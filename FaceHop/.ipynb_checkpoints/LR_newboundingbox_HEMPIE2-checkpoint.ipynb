{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2020.07.12\n",
    "# HEMPIE2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pickle\n",
    "import sys\n",
    "from os.path import dirname\n",
    "from os import getcwd\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from framework.pixelhop2 import PixelHopPP_Unit\n",
    "from framework.util import get_gender_label, get_image_array, myStandardScaler, Generate_feature, MaxPooling\n",
    "import cv2\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy.random import RandomState\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "n1 =18\n",
    "n2 = 13\n",
    "n3 = 11\n",
    "n_comp = 20\n",
    "standardize = False\n",
    "energy_th = 0.0005\n",
    "num_of_train_pixelhop = 4000\n",
    "\n",
    "foldnum = 1\n",
    "\n",
    "def Generate_feature_single_img(x, n_comp, pca_list=[], hop=1, loc={'1': [[0, 0, 10 ,12],[0, 16, 10, 28], [7, 9, 18, 19], [17,5, 25, 23]],\n",
    "                                    '2':[[0, 0, 4, 10], [6, 0, 10, 10], [0, 3, 10, 7]]}):\n",
    "    # old '2':[[0, 0, 4, 10], [4, 1, 10, 9]]\n",
    "    fea_in_loc = []\n",
    "    lenn = len(pca_list)\n",
    "    for i in range(len(loc[str(hop)])):\n",
    "        l = loc[str(hop)][i]\n",
    "        tmp_fea = []\n",
    "        tmp_pca = []\n",
    "        for k in range(x.shape[-1]):\n",
    "            tmp = x[:, l[0]:l[2], l[1]:l[3], k].reshape(x.shape[0], -1)\n",
    "            if lenn == 0:\n",
    "                pca = PCA(n_components=n_comp)\n",
    "                pca.fit(tmp)\n",
    "                tmp_pca.append(pca)\n",
    "            else:\n",
    "                pca = pca_list[i][k]\n",
    "            tmp = pca.transform(tmp)\n",
    "            tmp_fea.append(tmp)\n",
    "        fea_in_loc.append(np.concatenate(tmp_fea, axis=1))\n",
    "        if lenn == 0:\n",
    "            pca_list.append(tmp_pca)\n",
    "    return fea_in_loc, pca_list\n",
    "\n",
    "def feature_extraction(X_train, X_test):\n",
    "    allpatches = np.concatenate((X_train, X_test),axis=0)\n",
    "    S = [len(X_train), len(X_test)]\n",
    "    \n",
    "    saab, outtrainsaab1, kernel_filter, falttened = PixelHopPP_Unit(allpatches[0:num_of_train_pixelhop], num_kernels=n1, saab=None, window=5, stride=1, train=True, energy_th=energy_th, ch_decoupling=False, ch_energies=None, kernel_filter=[])\n",
    "    #Apply saab transfrom to the training data\n",
    "    _, out1, _, _ = PixelHopPP_Unit(allpatches, num_kernels=n1, saab=saab, window=5, stride=1, train=False, ch_decoupling=False, kernel_filter=kernel_filter)\n",
    "    out1ave = MaxPooling(out1)\n",
    "    print(\"       <INFO> Hop1 #Nodes: %s\"%(out1.shape[-1]))\n",
    "    saab, outtrainsaab2, kernel_filter, falttened = PixelHopPP_Unit(out1ave[0:num_of_train_pixelhop], num_kernels=n2, saab=None, window=5, stride=1, train=True, energy_th=energy_th, ch_decoupling=True, ch_energies=falttened, kernel_filter=[])\n",
    "    _, out2, _, _ = PixelHopPP_Unit(out1ave, num_kernels=n2, saab=saab, window=5, stride=1, train=False, ch_decoupling=True, kernel_filter=kernel_filter)\n",
    "    out2ave = MaxPooling(out2)\n",
    "    print(\"       <INFO> Hop2 #Nodes: %s\"%(out2.shape[-1]))\n",
    "    saab, _, kernel_filter, falttened = PixelHopPP_Unit(out2ave[0:num_of_train_pixelhop], num_kernels=n3, saab=None, window=5, stride=1, train=True, energy_th=energy_th, ch_decoupling=True, ch_energies=falttened, kernel_filter=[])\n",
    "    _, out3, _, _ = PixelHopPP_Unit(out2ave,num_kernels=n3,saab=saab,window=5,stride=1,train=False,ch_decoupling=True,kernel_filter=kernel_filter)\n",
    "    print(\"       <INFO> Hop2 #Nodes: %s\"%(out3.shape[-1]))\n",
    "\n",
    "    out1_train, out1_test= out1[0:S[0]], out1[S[0]:S[1]+S[0]]\n",
    "    out2_train, out2_test= out2[0:S[0]], out2[S[0]:S[1]+S[0]]\n",
    "    out3_train, out3_test= out3[0:S[0]], out3[S[0]:S[1]+S[0]]\n",
    "\n",
    "    \n",
    "    out1_train, p = Generate_feature_single_img(out1_train, n_comp, pca_list=[], hop=1)\n",
    "    out1_test, _ = Generate_feature_single_img(out1_test, n_comp, pca_list=p, hop=1)\n",
    "\n",
    "    out2_train, p = Generate_feature_single_img(out2_train, n_comp, pca_list=[], hop=2)\n",
    "    out2_test, _ = Generate_feature_single_img(out2_test, n_comp, pca_list=p, hop=2)\n",
    "\n",
    "    out3_train = out3_train.reshape(out3_train.shape[0], -1)\n",
    "    out3_test = out3_test.reshape(out3_test.shape[0], -1)  \n",
    "    \n",
    "    return [out1_train, out2_train, [out3_train]], [out1_test, out2_test, [out3_test]]\n",
    "\n",
    "def LR(x_train, y_train, x_test, y_test):\n",
    "    #clf = SVC(gamma='auto', probability=True)\n",
    "    #clf = LinearRegression()\n",
    "    clf = LogisticRegression(n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"     <INFO> train acc: %s\"%(clf.score(x_train, y_train)))\n",
    "    print(\"     <INFO> test acc: %s\"%(clf.score(x_test, y_test)))\n",
    "    return clf.predict_proba(x_train), clf.predict_proba(x_test)\n",
    "\n",
    "def SVM(x_train, y_train, x_test, y_test):\n",
    "    clf = SVC(gamma='auto', probability=True)\n",
    "    #clf = LinearRegression()\n",
    "    #clf = LogisticRegression()\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"     <INFO> train acc: %s\"%(clf.score(x_train, y_train)))\n",
    "    print(\"     <INFO> test acc: %s\"%(clf.score(x_test, y_test)))\n",
    "    return clf.predict_proba(x_train), clf.predict_proba(x_test)\n",
    "    \n",
    "def Hop3_classification(train_val_feature_list, test_feature_list, train_val_labels, test_labels):\n",
    "    Hop3_train_val_feature = train_val_feature_list[-1]\n",
    "    Hop3_test_feature = test_feature_list[-1]\n",
    "    clf_SVM = SVC(gamma='auto', probability=True).fit(Hop3_train_val_feature, train_val_labels)\n",
    "    print('Train acc:', clf_SVM.score(Hop3_train_val_feature, train_val_labels))\n",
    "    print('Test acc:', clf_SVM.score(Hop3_test_feature, test_labels), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_aug(x):\n",
    "    new_x = []\n",
    "    for i in range(len(x)):\n",
    "        new_x.append(cv2.flip(x[i], 1).reshape(x.shape[1], x.shape[2], -1))\n",
    "    return np.array(new_x)\n",
    "\n",
    "def cwPCA(x, eng_percent):\n",
    "    pca = PCA(n_components=500)\n",
    "    x = pca.fit_transform(x)\n",
    "    ratio = np.cumsum(pca.explained_variance_ratio_) >= eng_percent\n",
    "    n_comp = np.argmax(ratio)\n",
    "    #print(n_comp, \" compontents retained!\")\n",
    "    x = x[:, :n_comp]\n",
    "    dis = euclidean_distances(x, x)+1000*np.eye(len(x))\n",
    "    return dis\n",
    "\n",
    "def pca_aug(x, eng_percent=0.9):\n",
    "    xx = x.copy()\n",
    "    x = x.reshape(x.shape[0], -1, 1)/255\n",
    "    dis0 = cwPCA(x[:,:,0], eng_percent)\n",
    "    idx = np.argmin(dis0, axis=1)\n",
    "    new_x = []\n",
    "    ct = 0\n",
    "    for i in range(len(xx)):\n",
    "        tmp = xx[i]/2 + xx[idx[i]] / 2\n",
    "        #tmp = cv2.equalizeHist(tmp.astype(np.uint8))\n",
    "        new_x.append(tmp)\n",
    "        if ct > 0:\n",
    "            plt.imshow(tmp[:,:])\n",
    "            plt.title('mean')\n",
    "            plt.show()\n",
    "            plt.imshow(xx[i,:,0])\n",
    "            plt.title('raw')\n",
    "            plt.show()\n",
    "            ct -= 1\n",
    "    return np.array(new_x).reshape(xx.shape[0], xx.shape[1], xx.shape[2], -1)\n",
    "\n",
    "def data_aug(xf):\n",
    "    x1 = flip_aug(xf)\n",
    "    print(x1.shape)\n",
    "    x2 = pca_aug(xf)\n",
    "    print(x2.shape)\n",
    "    xf = np.concatenate((xf, x1, x2), axis=0)\n",
    "    print('after aug', xf.shape)\n",
    "    return xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29860 29860\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import cv2  \n",
    "\n",
    "datasetPath= \"/mnt/yifan/SavedHEMPIE2/s1/\"  \n",
    "file1 = open('../data/subject_list.txt', 'r') \n",
    "Lines = file1.readlines() \n",
    "  \n",
    "dictionary={}\n",
    "for line in Lines:\n",
    "    #print(line)\n",
    "    try:\n",
    "        identity_num=line.split(' ')[0]\n",
    "        gender=line.split(' ')[2]\n",
    "        dictionary[identity_num]= 0 if gender=='Male' else 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "data=[]\n",
    "labels=[]\n",
    "identities=glob(datasetPath+\"*\")\n",
    "for identity in identities:\n",
    "    identityId = os.path.basename(identity).split('.')[0].split('-')[1]\n",
    "    while len(identityId)<3:\n",
    "        identityId='0'+identityId\n",
    "    try:\n",
    "        img = cv2.resize(cv2.imread(identity)[:,:,0],(32,32))\n",
    "    except:\n",
    "        continue\n",
    "    data.append(img)\n",
    "    labels.append(dictionary[identityId])\n",
    "data = np.array(data).reshape(-1, 32, 32, 1)\n",
    "labels = np.array(labels)\n",
    "print(len(data), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male: 20870 female: 8990\n"
     ]
    }
   ],
   "source": [
    "x = data.copy()\n",
    "y = labels.copy()\n",
    "print('male:', len(y[y==0]), 'female:', len(y[y==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29860, 32, 32, 1) (29860,)\n",
      "(23888, 32, 32, 1) (23888,)\n",
      "(7192, 32, 32, 1)\n",
      "(7192, 32, 32, 1)\n",
      "after aug (21576, 32, 32, 1)\n",
      "(45464, 32, 32, 1) (45464,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, labels.shape)\n",
    "train_images, test_images, y, yt = train_test_split(data, labels, test_size=0.2, stratify=labels)\n",
    "#train_images, test_images, y, yt = train_test_split(train_images, y, test_size=0.2, stratify=y)\n",
    "\n",
    "print(train_images.shape, y.shape)\n",
    "\n",
    "x_aug = data_aug(train_images[y==1])\n",
    "train_images = np.concatenate((train_images, x_aug), axis=0)\n",
    "y = np.concatenate((y, np.ones(len(x_aug))), axis=0)\n",
    "print(train_images.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45464, 32, 32, 1) (5972, 32, 32, 1)\n",
      "       <INFO> Hop1 #Nodes: 18\n",
      "       <INFO> Hop2 #Nodes: 117\n",
      "       <INFO> Hop2 #Nodes: 186\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape, test_images.shape)\n",
    "x, xt = feature_extraction(train_images, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     <INFO> train acc: 0.8512449410522611\n",
      "     <INFO> test acc: 0.7965505693235098\n",
      "     <INFO> train acc: 0.8433705789195848\n",
      "     <INFO> test acc: 0.7896851975887474\n",
      "     <INFO> train acc: 0.766078655639627\n",
      "     <INFO> test acc: 0.6557267247153382\n",
      "     <INFO> train acc: 0.745095020235791\n",
      "     <INFO> test acc: 0.6421634293369055\n",
      "\n",
      "     <INFO> train acc: 0.9552613056484252\n",
      "     <INFO> test acc: 0.9196249162759544\n",
      "     <INFO> train acc: 0.9222461727960585\n",
      "     <INFO> test acc: 0.8804420629604822\n",
      "     <INFO> train acc: 0.9530617631532641\n",
      "     <INFO> test acc: 0.9132618888144675\n",
      "\n",
      "     <INFO> train acc: 0.8872734471229984\n",
      "     <INFO> test acc: 0.8414266577361018\n",
      "\n",
      "\n",
      " ensemble\n",
      "\n",
      "     <INFO> train acc: 0.9751231743797291\n",
      "     <INFO> test acc: 0.9519423978566645\n"
     ]
    }
   ],
   "source": [
    "px, pxt = [], []\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[i])):\n",
    "        a, b = LR(x[i][j], y, xt[i][j], yt)\n",
    "        px.append(a)\n",
    "        pxt.append(b)\n",
    "    print()\n",
    "print('\\n ensemble')\n",
    "px = np.concatenate(px, axis=1)\n",
    "pxt = np.concatenate(pxt, axis=1)\n",
    "print('')\n",
    "a, b = LR(px, y, pxt, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR all\n",
      "     <INFO> train acc: 0.9751231743797291\n",
      "     <INFO> test acc: 0.9519423978566645\n",
      "LR hop2+hop3\n",
      "     <INFO> train acc: 0.9747712475805033\n",
      "     <INFO> test acc: 0.9511051574012056\n",
      "SVM all\n",
      "     <INFO> train acc: 0.9753211332042935\n",
      "     <INFO> test acc: 0.9539517749497656\n",
      "SVM hop2+hop3\n",
      "     <INFO> train acc: 0.9749472109801162\n",
      "     <INFO> test acc: 0.949430676490288\n"
     ]
    }
   ],
   "source": [
    "print('LR all')\n",
    "a, b = LR(px[:,:], y, pxt[:,:], yt)\n",
    "print('LR hop2+hop3')\n",
    "a, b = LR(px[:,8:], y, pxt[:,8:], yt)\n",
    "print('SVM all')\n",
    "a, b = SVM(px[:,:], y, pxt[:,:], yt)\n",
    "print('SVM hop2+hop3')\n",
    "a, b = SVM(px[:,8:], y, pxt[:,8:], yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "with open('/mnt/yifan/face_gender_fea_HEMPIE2.pkl', 'wb') as f:\n",
    "    #d = pickle.load(f)\n",
    "    pickle.dump({'x':x,'y':y,'xt':xt, 'yt':yt},f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
